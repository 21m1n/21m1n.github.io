<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on De Omnibus Dubitandum</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on De Omnibus Dubitandum</description>
    <generator>Hugo -- 0.142.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 07:19:42 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Asyncio</title>
      <link>http://localhost:1313/posts/asyncio/</link>
      <pubDate>Fri, 31 Jan 2025 07:19:42 +0800</pubDate>
      <guid>http://localhost:1313/posts/asyncio/</guid>
      <description>&lt;p&gt;In the era of large language models and AI applications, handling concurrent operations efficiently has become more crucial than ever. Inefficient handling can severely impact application performance and user experience - a 500ms delay in API response can cascade into multi-second waits for end users. In AI applications, this is particularly critical when managing multiple LLM interactions, such as real-time inferences, and so on.Â &lt;/p&gt;
&lt;p&gt;Python offers several approaches to handle concurrent operations, including threading, multiprocessing, and asynchronous I/O. While threading works well for I/O-bound tasks and multiprocessing suits CPU-intensive operations, asyncio provides an elegant solution specifically designed for managing parallel operations efficiently. Asyncio particularly shines in LLM applications by enabling non-blocking API calls, efficient stream processing, and responsive user interactions. Without async capabilities, these operations would block each other, creating noticeable delays and degrading user experience. Asyncio provides Python developers the tools to manage these parallel operations efficiently, enabling smooth interaction with LLMs while maximizing system resources.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding DSPy</title>
      <link>http://localhost:1313/posts/my-new-post/</link>
      <pubDate>Thu, 30 Jan 2025 23:46:40 +0800</pubDate>
      <guid>http://localhost:1313/posts/my-new-post/</guid>
      <description>&lt;p&gt;One of the main challenges we face while developing AI applications is prompt engineering. As developers, we often find ourselves in a cycle of trial and error: writing a prompt, testing it against various inputs, analyzing the outputs, and then tweaking the prompt based on the results. This iterative process requires meticulous tracking of prompt versions, understanding how different phrasings affect model behavior, and maintaining consistency across similar tasks. Managing this complexity becomes even more challenging when we need to chain multiple prompts together or handle different edge cases. This approach, while workable for simple applications, becomes increasingly unsustainable as applications grow in complexity.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
